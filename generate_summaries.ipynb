{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "550c1372-a42d-4eeb-8d8c-e28a995ca5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "from huggingface_hub import login\n",
    "import gradio as gr\n",
    "from IPython.display import Markdown, display\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69b9bbbe-4f7d-4119-a1b2-4d0832545f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1644857-8567-48d8-aa35-f72d3d7c417c",
   "metadata": {},
   "source": [
    "## Importing the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "543a2fc8-efeb-4005-ba8d-9f0c399d3f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj\n",
      "AIz\n",
      "sk-o\n",
      "sk-or-v\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "claude_api_key = os.getenv('CLAUDE_API_KEY')\n",
    "MODEL_OPENAI = 'gpt-4o-mini'\n",
    "MODEL_GEMINI = 'gemini-1.5-flash'\n",
    "MODEL_DEEPSEEK = \"deepseek/deepseek-chat\"\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "MODEL_CLAUDE_SONNET = \"anthropic/claude-3.7-sonnet\"\n",
    "openai = OpenAI()\n",
    "\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "gemini = genai.GenerativeModel(MODEL_GEMINI)\n",
    "\n",
    "deepseek = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=deepseek_api_key,\n",
    ")\n",
    "\n",
    "claude = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=claude_api_key,\n",
    ")\n",
    "\n",
    "print(openai_api_key[:7])\n",
    "print(gemini_api_key[:3])\n",
    "print(deepseek_api_key[:4])\n",
    "print(claude_api_key[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2626c-31a4-4d01-854e-82331f47d2c3",
   "metadata": {},
   "source": [
    "## System and User Prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8f8037e-cd9d-4f12-8241-8864ca289a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a business analyst at a company that provides security systems and smart home solutions. \n",
    "Read the following chat transcripts between customers and support agents. \n",
    "Your task is to extract general insights such as:\n",
    "- recurring problems,\n",
    "- product feedback,\n",
    "- service issues,\n",
    "- and any trends that could help ADT improve customer experience or business strategy.\n",
    "Your summary should be factual, concise, and suitable for executives.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Chat 1:\n",
    "Customer: My door sensor keeps showing low battery even after I replace it.\n",
    "Agent: I understand, let me check if it’s a known issue with the firmware...\n",
    "\n",
    "Chat 2:\n",
    "Customer: My smart camera goes offline every night.\n",
    "Agent: That sounds like a router scheduling setting, let’s check it...\n",
    "\n",
    "\n",
    "Please summarize all the insights and patterns you can infer from the above chats.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14efe6a1-86cd-4107-8041-68453429e2c2",
   "metadata": {},
   "source": [
    "## OpenAI's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cfe2bcd-dd28-453c-9b2c-0051f27c8d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Insights and Patterns from Customer Support Chats:**\n",
       "\n",
       "1. **Recurring Problems:**\n",
       "   - **Door Sensor Battery Issues:** Multiple customers are experiencing low battery notifications for door sensors even after battery replacement, indicating a potential firmware or hardware issue.\n",
       "   - **Smart Camera Connectivity:** Customers report smart cameras going offline regularly, suggesting possible connectivity issues related to router settings or device compatibility.\n",
       "\n",
       "2. **Product Feedback:**\n",
       "   - There is a need for enhanced reliability in battery monitoring for door sensors and improved connectivity for smart cameras, as these issues could lead to customer dissatisfaction and reduced trust in product performance.\n",
       "\n",
       "3. **Service Issues:**\n",
       "   - The support agents are required to troubleshoot potential underlying issues, such as firmware problems and router settings, which may indicate a gap in product documentation or user guidance.\n",
       "\n",
       "4. **Trends for Improvement:**\n",
       "   - **Firmware Updates:** Investigating and addressing firmware issues for door sensors could enhance product reliability.\n",
       "   - **User Education:** Providing clearer instructions on router settings and device management could help customers avoid connectivity issues with smart cameras.\n",
       "   - **Proactive Support:** Implementing a system to notify customers of known issues and solutions could improve the overall customer experience and reduce the volume of support inquiries.\n",
       "\n",
       "These insights suggest areas for product development, customer education, and service enhancement to improve overall customer satisfaction and loyalty."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(\n",
    "    model=MODEL_OPENAI,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content.strip()\n",
    "display(Markdown(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68c3e1-018f-438b-93e3-5a02d5355010",
   "metadata": {},
   "source": [
    "## Gemini's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c639667-1a28-48e1-98f0-c3e5cbf346e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Analysis of Customer Chat Transcripts:\n",
       "\n",
       "**Recurring Problems:**\n",
       "\n",
       "* **Faulty/Inconsistent Sensor Battery Readings:**  At least one customer reports inaccurate low battery readings on a door sensor despite battery replacement, suggesting a potential hardware or firmware issue.\n",
       "\n",
       "* **Smart Camera Connectivity Issues:**  One customer experiences nightly offline periods with their smart camera, pointing to potential network configuration problems (router settings) or intermittent camera failures.\n",
       "\n",
       "\n",
       "**Product Feedback (Implicit):**\n",
       "\n",
       "* **Door Sensor Reliability:** The battery issue highlights concerns about the reliability and accuracy of the door sensor's reporting mechanism.\n",
       "* **Smart Camera Stability:** The reported connectivity issue suggests a need for improved camera network stability and possibly more robust error handling.\n",
       "\n",
       "\n",
       "**Service Issues:**\n",
       "\n",
       "* No direct service issues are evident in these short transcripts, but the need for troubleshooting suggests potential areas for improvement in product design or user documentation (e.g., clearer instructions on router settings affecting smart devices).\n",
       "\n",
       "\n",
       "**Trends and Recommendations:**\n",
       "\n",
       "* **Investigate Door Sensor Firmware/Hardware:** A deeper investigation into the root cause of the inaccurate battery readings in Chat 1 is crucial. This could involve firmware updates, hardware revisions, or improved battery detection algorithms.\n",
       "\n",
       "* **Improve Smart Camera Network Compatibility:** The camera's nightly offline periods indicate a need for improved documentation on network configuration, or potential design changes to enhance the camera's robustness to common router settings or network fluctuations.\n",
       "\n",
       "* **Proactive Communication:** Consider proactively informing customers about known issues (e.g., via app notifications or email) to prevent frustration.\n",
       "\n",
       "* **Enhance User Documentation:** Clearer and more comprehensive user manuals and in-app instructions are needed to address common technical hurdles.  This should particularly address network configuration for smart devices.\n",
       "\n",
       "The limited data prevents definitive conclusions, but these initial findings suggest areas warranting further investigation to improve both product quality and customer satisfaction.  More data is needed for more comprehensive analysis.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "response = gemini.generate_content(system_prompt + \"\\n\" + user_prompt)\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f920abf-f080-4cae-be91-c61bc7a4c52a",
   "metadata": {},
   "source": [
    "## Llama 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0bc6739-8262-4d7c-a7b9-542a0171b58a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RemoteProtocolError",
     "evalue": "Server disconnected without sending a response.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRemoteProtocolError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:231\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    230\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mServer disconnected without sending a response.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteProtocolError(msg)\n\u001b[32m    233\u001b[39m \u001b[38;5;28mself\u001b[39m._h11_state.receive_data(data)\n",
      "\u001b[31mRemoteProtocolError\u001b[39m: Server disconnected without sending a response.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRemoteProtocolError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m messages=[\n\u001b[32m      2\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_prompt},\n\u001b[32m      3\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: user_prompt}\n\u001b[32m      4\u001b[39m     ]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_LLAMA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m reply = response[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      8\u001b[39m display(Markdown(reply))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/ollama/_client.py:333\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    290\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    291\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    298\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    299\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    300\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    302\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    331\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/ollama/_client.py:178\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/ollama/_client.py:118\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    117\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     r.raise_for_status()\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.5/lib/python3.11/contextlib.py:155\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    153\u001b[39m     value = typ()\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mRemoteProtocolError\u001b[39m: Server disconnected without sending a response."
     ]
    }
   ],
   "source": [
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "reply = response['message']['content']\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cd2691-4e2c-483f-bafd-a20aca71a1e6",
   "metadata": {},
   "source": [
    "## Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b3766f-4d0d-4f85-b97b-da45704686e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Insights from Chat Transcripts:**\n",
       "\n",
       "1. **Recurring Problems:**\n",
       "   - **Battery Issues:** Customers are experiencing persistent low battery notifications with ADT door sensors, even after replacing the batteries. This suggests a potential firmware or hardware issue.\n",
       "   - **Device Connectivity:** Smart cameras are frequently going offline, particularly at night, which may be linked to router scheduling settings or Wi-Fi stability.\n",
       "\n",
       "2. **Product Feedback:**\n",
       "   - **Door Sensor Reliability:** The consistent low battery issue with door sensors indicates a need for either firmware updates or hardware improvements to enhance reliability.\n",
       "   - **Smart Camera Connectivity:** The smart camera’s offline problem highlights a possible need for better integration with common router settings or improved Wi-Fi compatibility.\n",
       "\n",
       "3. **Service Issues:**\n",
       "   - **Technical Support Response:** Agents are identifying potential causes (e.g., firmware checks, router settings), but recurring issues suggest a need for more proactive solutions or clearer troubleshooting guides for customers.\n",
       "\n",
       "4. **Trends:**\n",
       "   - **Firmware/Software Concerns:** Firmware appears to be a common factor in device malfunctions, warranting a review of device software updates.\n",
       "   - **Network Integration:** Connectivity issues point to a need for better compatibility with home network setups, including routers and Wi-Fi systems.\n",
       "\n",
       "**Key Recommendations:**\n",
       "- Investigate and address firmware issues with door sensors to resolve low battery notifications.\n",
       "- Enhance smart camera compatibility with router scheduling and Wi-Fi systems to prevent offline incidents.\n",
       "- Develop proactive troubleshooting support materials for common issues to improve customer self-service.\n",
       "- Consider a review of firmware and software updates across devices to ensure optimal performance and reliability."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "response = deepseek.chat.completions.create(\n",
    "    model=MODEL_DEEPSEEK,\n",
    "    messages=messages\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e57ba-249b-451c-817f-93383a29473d",
   "metadata": {},
   "source": [
    "## Gradio (take inputs for system prompt and user prompt from users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd66cff1-9797-4e9e-9352-16cfb48974dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def generate_summary(llm_choice, system_prompt, user_prompt):\n",
    "    if llm_choice == \"OPENAI\":\n",
    "        response = openai.chat.completions.create(\n",
    "            model=MODEL_OPENAI,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    elif llm_choice == \"CLAUDE\":\n",
    "        try:\n",
    "            response = claude.messages.create(\n",
    "                model=\"anthropic/claude-3.7-sonnet\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"{system_prompt}\\n\\n{user_prompt}\"}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                extra_headers={\n",
    "                    \"HTTP-Referer\": \"https://example.com\",  # required by OpenRouter\n",
    "                    \"X-Title\": \"ADT Transcript Insight Generator\"\n",
    "                }\n",
    "            )\n",
    "            return response.message.content[0].text.strip()\n",
    "        except Exception as e:\n",
    "            return f\"Claude failed: {str(e)}\"\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    elif llm_choice == \"GEMINI\":\n",
    "        response = gemini.generate_content(system_prompt + \"\\n\" + user_prompt)\n",
    "        return response.text\n",
    "\n",
    "    elif llm_choice == \"DEEPSEEK\":\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        response = deepseek.chat.completions.create(\n",
    "            model=MODEL_DEEPSEEK,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    else:\n",
    "        return \"Invalid model selection.\"\n",
    "\n",
    "# Create the interface\n",
    "gr.Interface(\n",
    "    fn=generate_summary,\n",
    "    inputs=[\n",
    "        gr.Dropdown(choices=[\"OPENAI\", \"GEMINI\", \"DEEPSEEK\", \"CLAUDE\"], label=\"Select LLM\"),\n",
    "        gr.Textbox(lines=6, label=\"System Prompt\"),\n",
    "        gr.Textbox(lines=12, label=\"User Prompt\"),\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"LLM Response\"),\n",
    "    title=\"ADT Transcript Insight Generator\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22655ed-bb59-4217-9638-1f5a17ce134b",
   "metadata": {},
   "source": [
    "## Gradio (pre-defined system and user prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d52a0749-c9d2-402e-a175-82ca0d3f6a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/harshakparmar/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/gradio/queueing.py\", line 715, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harshakparmar/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harshakparmar/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/gradio/blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harshakparmar/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1675, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harshakparmar/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/gradio/utils.py\", line 735, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harshakparmar/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/gradio/utils.py\", line 729, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harshakparmar/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harshakparmar/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/harshakparmar/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harshakparmar/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/gradio/utils.py\", line 712, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/harshakparmar/Desktop/LLM-based_projects/venv/lib/python3.11/site-packages/gradio/utils.py\", line 873, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/wq/7l38g16j4lbbdh6l6fgr25x00000gn/T/ipykernel_6941/2642795350.py\", line 18, in generate_summary\n",
      "    response = claude.chat.completetions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Chat' object has no attribute 'completetions'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def generate_summary(llm_choice):\n",
    "    yield \"⏳ Generating response from \" + llm_choice + \"...\"\n",
    "    \n",
    "    if llm_choice == \"OPENAI\":\n",
    "        response = openai.chat.completions.create(\n",
    "            model=MODEL_OPENAI,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "\n",
    "    elif llm_choice == \"CLAUDE SONNET 3.7\":\n",
    "        response = claude.chat.completetions.create(\n",
    "            model=MODEL_CLAUDE_SONNET,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature = 0.3\n",
    "        )\n",
    "\n",
    "        result = response.choices[0].message.content.strip()\n",
    "\n",
    "    elif llm_choice == \"GEMINI\":\n",
    "        response = gemini.generate_content(system_prompt + \"\\n\" + user_prompt)\n",
    "        result = response.text\n",
    "\n",
    "    elif llm_choice == \"DEEPSEEK\":\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        response = deepseek.chat.completions.create(\n",
    "            model=MODEL_DEEPSEEK,\n",
    "            messages=messages\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "    elif llm_choice == 'LLAMA':\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "        result = response['message']['content']\n",
    "        \n",
    "\n",
    "\n",
    "    else:\n",
    "        result = \"❌ Invalid model selection.\"\n",
    "\n",
    "    yield result\n",
    "\n",
    "# Interface with streaming output\n",
    "gr.Interface(\n",
    "    fn=generate_summary,\n",
    "    inputs=gr.Dropdown(choices=[\"OPENAI\", \"GEMINI\", \"DEEPSEEK\", \"LLAMA\", \"CLAUDE SONNET 3.7\"], label=\"Select LLM\"),\n",
    "    outputs=gr.Markdown(label=\"LLM Response\"),\n",
    "    title=\"ADT Transcript Insight Generator\",\n",
    "    live=True\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe10e7-c121-49fc-89ac-531287702516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
